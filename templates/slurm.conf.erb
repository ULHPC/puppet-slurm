# -*- mode: conf; -*-
###################################################################################
# slurm.conf -- General Slurm configuration information for the '<%= scope['slurm::clustername'] %>'
# cluster
###################################################################################
# Documentation:
# see https://slurm.schedmd.com/slurm.conf.html
#
###################################################################################
# /!\ DO NOT EDIT THIS FILE: It has been automatically generated by Puppet.
#  In particular, any further changes will be overwritten at the next puppet
#  invocation
###################################################################################
#

ClusterName=<%= scope['slurm::clustername'] %>

############################
###  Management Policies ###
############################
# - Location of controllers, backups, logs, state info
# - Authentication
# - Cryptographic tool
# - Checkpoint
# - Logging
# - Process tracking
# - Timers
# - Power saving on idle nodes
###

ControlMachine=<%= scope['slurm::controlmachine'] %>
<% if scope['slurm::controladdr'].empty? -%>
#ControlAddr=
<% else -%>
ControlAddr=<%= scope['slurm::controladdr'] %>
<% end -%>
<% if scope['slurm::backupcontroller'].empty? -%>
#BackupController=
<% else -%>
BackupController=<%= scope['slurm::backupcontroller'] %>
<% end -%>
<% if scope['slurm::backupaddr'].empty? -%>
#BackupAddr=
<% else -%>
BackupAddr=<%= scope['slurm::backupaddr'] %>
<% end -%>


#
AuthType=auth/<%= scope['slurm::authtype'] %>
<% unless scope['slurm::authinfo'].empty? -%>
AuthInfo=auth/<%= scope['slurm::authinfo'] %>
<% end -%>
CryptoType=crypto/<%= scope['slurm::cryptotype'] %>

<% if scope['slurm::checkpointtype'] == 'none' -%>
#CheckpointType=checkpoint/none
<% else -%>
CheckpointType=checkpoint/<%= scope['slurm::checkpointtype'] %>
<% end -%>

DisableRootJobs=<%= scope['slurm::disablerootjobs'] ? 'YES' : 'NO' %>
EnforcePartLimits=<%= scope['slurm::enforcepartlimits'] %>
#FirstJobId=1
#MaxJobId=999999
<% if scope['slurm::grestypes'].empty? -%>
#GresTypes=
<% else -%>
GresTypes=<%= scope['slurm::grestypes'].join(',') %>
<% end -%>
#GroupUpdateForce=0
#GroupUpdateTime=600
<% if scope['slurm::jobcheckpointdir'].empty? -%>
#JobCheckpointDir=/var/lib/slurm/checkpoint
<% else -%>
JobCheckpointDir=<%= scope['slurm::jobcheckpointdir'] %>
<% end -%>
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
#JobFileAppend=0
JobRequeue=<%= scope['slurm::jobrequeue'] ? 1 : 0 %>
#KillOnBadExit=0
TmpFS=<%= scope['slurm::tmpfs'] %>
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
UsePAM=<%= scope['slurm::use_pam'] ? 1 : 0 %>

SlurmUser=<%= scope['slurm::params::username'] %>
#SlurmdUser=root

MailProg=<%= scope['slurm::mailprog'] %>
MailDomain=<%= scope['slurm::maildomain'] %>
RebootProgram=/sbin/reboot
ReturnToService=<%= scope['slurm::returntoservice'] %>
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=<%= scope['slurm::slurmctldport'] %>
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=<%= scope['slurm::slurmdport'] %>
SlurmdSpoolDir=/var/lib/slurmd
<% if scope['slurm::srunepilog'].empty? -%>
#SrunEpilog=
<% else -%>
SrunEpilog=<%= scope['slurm::srunepilog'] %>
<% end -%>
<% if scope['slurm::srunprolog'].empty? -%>
#SrunProlog=
<% else -%>
SrunProlog=<%= scope['slurm::srunprolog'] %>
<% end -%>
SrunPortRange=<%= scope['slurm::srunportrange'] %>

### LOGGING
<% if scope['slurm::debugflags'].empty?  -%>
#DebugFlags=
<% else -%>
DebugFlags=<%= scope['slurm::debugflags'].join(',') %>
<% end -%>
<% if scope['slurm::jobcomphost'].empty?  -%>
#JobCompHost=
<% else -%>
JobCompHost=<%= scope['slurm::jobcomphost'] %>
<% end -%>
<% if scope['slurm::jobcomploc'].empty? -%>
#JobCompLoc=
<% else -%>
JobCompLoc=<%= scope['slurm::jobcomploc'] %>
<% end -%>
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/<%= scope['slurm::jobcomptype'] %>
#JobCompUser=
#JobContainerType=job_container/<%= scope['slurm::jobcontainertype'] %>
SlurmctldDebug=<%= scope['slurm::slurmctlddebug'] %>
SlurmctldLogFile=<%= scope['slurm::params::logdir'] %>/slurmctld.log
SlurmdDebug=<%= scope['slurm::slurmddebug'] %>
SlurmdLogFile=<%= scope['slurm::params::logdir'] %>/slurmd.log
LogTimeFormat=<%= scope['slurm::logtimeformat'] %>

<% if scope['slurm::slurmddebugsyslog'].empty? -%>
#SlurmdSyslogDebug=
<% else -%>
SlurmdSyslogDebug=<%= scope['slurm::slurmddebugsyslog']%>
<% end -%>

<% if scope['slurm::slurmctlddebugsyslog'].empty? -%>
#SlurmctldSyslogDebug=
<% else -%>
SlurmctldSyslogDebug=<%= scope['slurm::slurmctlddebugsyslog']%>
<% end -%>

#SlurmSchedLogFile=
#SlurmSchedLogLevel=

### Plugins
# PluginDir: Identifies the places in which to look for Slurm plugins

# JobSubmitPlugins: A comma delimited list of job submission plugins to be used.
<% if scope['slurm::jobsubmitplugins'].empty? -%>
# JobSubmitPlugins=lua
<% else -%>
JobSubmitPlugins=<%= scope['slurm::jobsubmitplugins'].join(',') %>
<% end -%>


### TIMERS

<% if scope['slurm::batchstarttimeout'] == 10 -%>
#BatchStartTimeout=10
<% else -%>
BatchStartTimeout=<%= scope['slurm::batchstarttimeout'] %>
<% end -%>
<% if scope['slurm::completewait'] > 0 -%>
#CompleteWait=0
<% else -%>
CompleteWait=<%= scope['slurm::completewait'] %>
<% end -%>

#EpilogMsgTime=2000
GetEnvTimeout=<%= scope['slurm::getenvtimeout'] %>
<% if scope['slurm::healthcheckprogram'].empty? %>
#HealthCheckProgram=/usr/sbin/nhc
#HealthCheckInterval=30
#HealthCheckInterval=300
#HealthCheckNodeState=ANY
<% else %>
HealthCheckProgram=<%= scope['slurm::healthcheckprogram'] %>
HealthCheckInterval=<%= scope['slurm::healthcheckinterval'] %>
HealthCheckNodeState=<%= scope['slurm::healthchecknodestate'] %>
<% end %>
InactiveLimit=<%= scope['slurm::inactivelimit'] %>
KillWait=<%= scope['slurm::killwait'] %>
MessageTimeout=<%= scope['slurm::messagetimeout'] %>
ResvOverRun=<%= scope['slurm::resvoverrun'] %>
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=<%= scope['slurm::slurmctldtimeout'] %>
SlurmdTimeout=<%= scope['slurm::slurmdtimeout'] %>
<% if scope['slurm::unkillablesteptimeout'] == 60 -%>
#UnkillableStepTimeout=60
<% else -%>
UnkillableStepTimeout=<%= scope['slurm::unkillablesteptimeout'] %>
<% end -%>
#VSizeFactor=0
Waittime=<%= scope['slurm::waittime'] %>

### POWER SAVE SUPPORT FOR IDLE NODES (optional)
<% if scope['slurm::suspendprogram'].empty? -%>
#SuspendProgram=
<% else -%>
SuspendProgram=<%= scope['slurm::suspendprogram'] %>
<% end -%>

<% if scope['slurm::resumeprogram'].empty? -%>
#ResumeProgram=
<% else -%>
ResumeProgram=<%= scope['slurm::resumeprogram'] %>
<% end -%>

<% if scope['slurm::suspendtimeout'] > 0 -%>
SuspendTimeout=<%= scope['slurm::suspendtimeout'] %>
<% else -%>
#SuspendTimeout=
<% end -%>

<% if scope['slurm::resumetimeout'] > 0 -%>
ResumeTimeout=<%= scope['slurm::resumetimeout'] %>
<% else -%>
#ResumeTimeout=
<% end -%>

<% if scope['slurm::resumerate'] > 0 -%>
ResumeRate=<%= scope['slurm::resumerate'] %>
<% else -%>
#ResumeRate=
<% end -%>

<% if scope['slurm::suspendexcnodes'].empty? -%>
#SuspendExcNodes=
<% else -%>
SuspendExcNodes=<%= scope['slurm::suspendexcnodes'] %>
<% end -%>

#SuspendExcParts=
#SuspendRate=
<% if scope['slurm::suspendtime'] > 0 -%>
SuspendTime=<%= scope['slurm::suspendtime'] %>
<% else -%>
#SuspendTime=
<% end -%>
##################
### Accounting ###
##################
<% if scope['slurm::acct_storageenforce'].empty? -%>
# AccountingStorageEnforce=qos,limits,associations
<% else -%>
AccountingStorageEnforce=<%= scope['slurm::acct_storageenforce'].join(',') %>
<% end -%>
AccountingStorageHost=<%= scope['slurm::accountingstoragehost'] %>
AccountingStorageLoc=slurm
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=
AccountingStoreJobComment=YES
AcctGatherEnergyType=acct_gather_energy/<%= scope['slurm::acct_gatherenergytype']%>
JobAcctGatherFrequency=<%= scope['slurm::jobacctgatherfrequency'] %>
JobAcctGatherType=jobacct_gather/<%= scope['slurm::jobacctgathertype'] %>
<% unless scope['slurm::jobacctgatherparams'].empty? -%>
JobAcctGatherParams=<%= scope['slurm::jobacctgatherparams'] %>
<% end -%>
<% if scope['slurm::licenses'].empty? -%>
#Licenses=foo*4,bar
<% else -%>
Licenses=<%= scope['slurm::licenses'] %>
<% end -%>



#MaxJobCount=5000
#MaxStepCount=40000
MaxTasksPerNode=<%= scope['slurm::maxtaskspernode'] %>
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/<%= scope['slurm::proctracktype'] %>
#PropagatePrioProcess=0
<% if scope['slurm::propagateresourcelimits'].empty? -%>
#PropagateResourceLimits=
<% else -%>
PropagateResourceLimits=<%= scope['slurm::propagateresourcelimits'].join(',') %>
<% end -%>
<% if scope['slurm::propagateresourcelimits_except'].empty? -%>
#PropagateResourceLimitsExcept=
<% else -%>
PropagateResourceLimitsExcept=<%= scope['slurm::propagateresourcelimits_except'].join(',') %>
<% end -%>
StateSaveLocation=<%= scope['slurm::statesavelocation'] %>


###########################
### Scheduling policies ###
###########################
# - Priority
# - Preemption
# - Backfill
# - job priority
###
FastSchedule=<%= scope['slurm::fastschedule'] %>
#SchedulerTimeSlice=30
SchedulerType=sched/<%= scope['slurm::schedulertype'] %>
<% if scope['slurm::schedulerparameters'].empty? -%>
#SchedulerParameters=
<% else -%>
SchedulerParameters=<%=  scope['slurm::schedulerparameters'].join(',') %>
<% end -%>

# Plugin used to identify which jobs can be preempted in order to start a pending job.
PreemptType=preempt/<%= scope['slurm::preempttype'] %>
PreemptMode=<%= scope['slurm::preemptmode'].join(',') %>

### JOB PRIORITY
# PriorityFlags: Comma delimited list of priority engine modifiers
<% if scope['slurm::priorityflags'].empty? -%>
#PriorityFlags=
<% else -%>
PriorityFlags=<%= scope['slurm::priorityflags'].join(',') %>
<% end -%>
<% if scope['slurm::prioritytype'].empty? -%>
#PriorityType=priority/basic
<% else -%>
PriorityType=priority/<%= scope['slurm::prioritytype'] %>
<% end -%>
<% if scope['slurm::prioritydecayhalflife'].empty? -%>
#PriorityDecayHalfLife=
<% else -%>
PriorityDecayHalfLife=<%= scope['slurm::prioritydecayhalflife'] %>
<% end -%>
#PriorityCalcPeriod=
PriorityFavorSmall=<%= scope['slurm::priorityfavorsmall'] ? 'YES' : 'NO' %>
PriorityMaxAge=<%= scope['slurm::prioritymaxage'] %>
PriorityUsageResetPeriod=<%= scope['slurm::priorityusageresetperiod'] %>
<% if scope['slurm::priorityweightage'] > 0 -%>
PriorityWeightAge=<%= scope['slurm::priorityweightage'] %>
<% else -%>
#PriorityWeightAge=
<% end -%>
<% if scope['slurm::priorityweightfairshare'] > 0 -%>
PriorityWeightFairshare=<%= scope['slurm::priorityweightfairshare'] %>
<% else -%>
#PriorityWeightFairshare=
<% end -%>
<% if scope['slurm::priorityweightjobsize'] > 0 -%>
PriorityWeightJobSize=<%= scope['slurm::priorityweightjobsize'] %>
<% else -%>
#PriorityWeightJobSize=
<% end -%>
<% if scope['slurm::priorityweightpartition'] > 0 -%>
PriorityWeightPartition=<%= scope['slurm::priorityweightpartition'] %>
<% else -%>
#PriorityWeightPartition=
<% end -%>
<% if scope['slurm::priorityweightqos'] > 0 -%>
PriorityWeightQOS=<%= scope['slurm::priorityweightqos'] %>
<% else -%>
#PriorityWeightQOS=
<% end -%>

###########################################
### Trackable RESources (TRES) policies ###
###########################################
# See https://slurm.schedmd.com/tres.html
# Define which TRES are to be tracked on the system in addition to the default
# billing, cpu, energy, memory and node
<% unless scope['slurm::accountingstoragetres'].empty? -%>
AccountingStorageTRES=<%= scope['slurm::accountingstoragetres'] %>
<% end -%>
<% if scope['slurm::priorityweighttres'].empty? -%>
# PriorityWeightTRES =
<% else -%>
PriorityWeightTRES=<%= scope['slurm::priorityweighttres'] %>
<% end -%>
# For each partition (or in the 'DEFAULT' partition setting), use the TRESBillingWeights
# option to define the billing weights of each TRES type that will be used in
# calculating the usage of a job.


###########################
### Allocation policies ###
###########################
# - Entire nodes or 'consumable resources'
# - Task Affinity (lock task on CPU)
# - Topology (minimum number of switches)
# - MPI type -- see https://slurm.schedmd.com/mpi_guide.html
# - Prolog / Epilog scripts
###
# Mechanism to be used to launch application tasks.
#LaunchType=launch/<%= scope['slurm::launchtype'] %>

SelectType=select/<%= scope['slurm::selecttype'] %>
SelectTypeParameters=<%= scope['slurm::selecttype_params'].join(',') %>
<% if scope['slurm::taskepilog'].empty? -%>
#TaskEpilog=
<% else -%>
TaskEpilog=<%= scope['slurm::taskepilog'] %>
<% end -%>
TaskPlugin=task/<%= scope['slurm::taskplugin'] %>
<% if scope['slurm::taskpluginparams'].empty? -%>
#TaskPluginParam=
<% else -%>
TaskPluginParam=<%= scope['slurm::taskpluginparams'].join(',') %>
<% end -%>
<% if scope['slurm::taskprolog'].empty? -%>
#TaskProlog=
<% else -%>
TaskProlog=<%= scope['slurm::taskprolog'] %>
<% end -%>

<% if scope['slurm::epilog'].empty? -%>
#Epilog=
<% else -%>
Epilog=<%= scope['slurm::epilog'] %>
<% end -%>
<% if scope['slurm::epilogslurmctld'].empty? -%>
#EpilogSlurmctld=
<% else -%>
EpilogSlurmctld=<%= scope['slurm::epilogslurmctld'] %>
<% end -%>
<% if scope['slurm::prolog'].empty? -%>
#Prolog=
<% else -%>
Prolog=<%= scope['slurm::prolog'] %>
<% end -%>
<% if scope['slurm::prologslurmctld'].empty? -%>
#PrologSlurmctld=
<% else -%>
PrologSlurmctld=<%= scope['slurm::prologslurmctld'] %>
<% end -%>
###  PrologFlags=contain: At job allocation time, use the ProcTrack plugin to
# create a job container on all allocated compute nodes. This container may be
# used for user processes not launched under Slurm control, for example the PAM
# module may place processes launch through a direct user login into this
# container. Setting the Contain implicitly sets the Alloc flag.
<% if scope['slurm::prologflags'].empty? -%>
#PrologFlags=contain
<% else -%>
PrologFlags=<%= scope['slurm::prologflags'].join(',') %>
<% end -%>

### MPI default
MpiDefault=<%= scope['slurm::mpidefault'] %>
<% if scope['slurm::mpiparams'].empty? -%>
#MpiParams=ports=#-#
<% else -%>
MpiParams=<%= scope['slurm::mpiparams'] %>
<% end -%>

<% if scope['slurm::custom_content'] -%>
### Custom content
<%= scope['slurm::custom_content'] %>

<% end -%>
###############################
### Network Characteristics ###
###############################
SwitchType=switch/<%=  scope['slurm::switchtype'] %>
# hierarchical network as described in 'topology.conf'
<% if scope['slurm::topology'].empty? %>
#TopologyPlugin=topology/tree
<% else -%>
TopologyPlugin=topology/<%= scope['slurm::topology'] %>
<% end -%>


#######################
### Node definition ###
#######################
# - Characteristics (sockets, cores, threads, memory, features)
# - Network addresses
CoreSpecPlugin=core_spec/<%= scope['slurm::corespecplugin'] %>
<% unless scope['slurm::cpufreqdef'].nil? -%>
CpuFreqDef=<%= scope['slurm::cpufreqdef'] %>
<% end -%>
<% unless scope['slurm::cpufreqgovernors'].empty? -%>
CpuFreqGovernors=<%= scope['slurm::cpufreqgovernors'].join(',') %>
<% end -%>


<% if scope['slurm::defmempercpu'] > 0 -%>
DefMemPerCPU=<%= scope['slurm::defmempercpu'] %>
<% else -%>
#DefMemPerCPU=4096
<% end -%>
<% if scope['slurm::maxmempercpu'] > 0 -%>
MaxMemPerCPU=<%= scope['slurm::maxmempercpu'] %>
<% else -%>
#MaxMemPerCPU=4196
<% end -%>

<% unless scope['slurm::defmempernode'].nil? -%>
DefMemPerNode=<%= scope['slurm::defmempernode'] %>
<% end -%>
<% unless scope['slurm::maxmempernode'].nil? -%>
MaxMemPerNode=<%= scope['slurm::maxmempernode'] %>
<% end -%>

#
# CPUs:    Number of logical processors on the node, default to Sockets*CoresPerSocket*ThreadsPerCore
# Sockets: Number of physical processor sockets/chips on the node
# CoresPerSocket: Number of cores in a single physical processor socket
# ThreadsPerCore: Number of logical threads in a single physical core
# RealMemory:     Size of real memory on the node in megabytes
# State:   State of the node with respect to the initiation of user jobs.
#          Acceptable values are "CLOUD", "DOWN", "DRAIN", "FAIL", "FAILING", "FUTURE" and "UNKNOWN"

<% scope['slurm::nodes'].each do |name, v| -%>
<% content = v.is_a?(Hash) ? v['content'] : v -%>
<% if v.is_a?(Hash) and v['comment'] -%>
# <%= v['comment'] %>
<% end -%>
NodeName=<%= name.ljust(12) %> <%= content %>
<% end -%>

##################################
### Partition / QOS definition ###
##################################
# - Set of nodes per partition
# - Sharing
# - Priority/preemption
###
<% scope['slurm::partitions'].each do |name, v| -%>
<%    content = v.is_a?(Hash) ? '' : v -%>
<%    if v.is_a?(Hash) -%>
<%       comment = v['comment'] ?  v['comment'] : "'#{name}' partition" -%>
<%       comment << " - Priority: #{v['priority']}" if v['priority']  -%>
<%       comment << "; preempt '#{v['preempt']}'"   if v['preempt']   -%>
# <%= comment %>
<%       content << "Nodes=#{v['nodes']} " unless v['nodes'].nil? -%>
<%       content << v['content'] unless v['content'].nil? -%>
<%       content << ' Default=YES' if (!v['default'].nil? and v['default'] and content !~ /Default=/) -%>
<%       content << ' Hidden=YES'  if (!v['hidden'].nil?  and v['hidden']  and content !~ /Hidden=/) -%>
<%       content << " State=#{v['state']}" unless (v['state'].nil? or content =~ /State=/) -%>
<%       content << " AllowAccounts=#{v['allowaccounts'].is_a?(Array) ? v['allowaccounts'].join(',') : v['allowaccounts']}" if v['allowaccounts'] -%>
<%       content << " AllowGroups=#{v['allowgroups'].is_a?(Array) ? v['allowgroups'].join(',') : v['allowgroups']}" if v['allowgroups'] -%>
<%       content << " AllowQos=#{v['allowqos'].is_a?(Array) ? v['allowqos'].join(',') : v['allowqos']}"      if v['allowqos'] -%>
<%       content << ' DisableRootJobs=YES'  if (!v['disablerootjobs'].nil?  and v['disablerootjobs']  and content !~ /DisableRootJobs=/) -%>
<%       content << " TRESBillingWeights=#{v['TRESbillingweights'].is_a?(Array) ? v['TRESbillingweights'].join(',') : v['TRESbillingweights']}" if v['TRESbillingweights'] -%>
<%    end -%>
<%    unless (content.nil? or content.empty?) -%>
PartitionName=<%= name.ljust(12)  %> <%= content %>
<%    end -%>
<% end -%>
