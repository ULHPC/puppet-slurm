################################################################################
# Time-stamp: <Sun 2019-02-03 14:47 svarrette>
#
# File::      <tt>params.pp</tt>
# Author::    UL HPC Team (hpc-sysadmins@uni.lu)
# Copyright:: Copyright (c) 2017 UL HPC Team
# License::   Apache-2.0
#
# ------------------------------------------------------------------------------
# == Class: slurm::params
#
# In this class are defined as default variables values that are used in all
# other slurm classes and definitions.
# This class should be included, where necessary, and eventually be enhanced
# with support for more Operating Systems.
#
# == Warnings
#
# /!\ Always respect the style guide available
# here[http://docs.puppetlabs.com/guides/style_guide]
#
# [Remember: No empty lines between comments and class definition]
#
class slurm::params {

  #### MODULE INTERNAL VARIABLES  #########
  # (Modify to adapt to unsupported OSes)
  #########################################
  $pre_requisite_packages = $::osfamily ? {
    'Redhat' => [
      'hwloc', 'hwloc-devel', 'hwloc-plugins', 'numactl', 'numactl-devel',
      'lua', 'lua-devel',
      'mysql-devel',
      'openssl', 'openssl-devel',
      'pam-devel',
      'perl-devel', 'perl-CPAN',
      'readline', 'readline-devel',
      'libX11-devel',
      'libssh2-devel',
    ],
    default => []
  }
  # Probably out of scope here, but useful
  $extra_packages = $::osfamily ? {
    'Redhat' => [
      'libibmad', 'rrdtool-devel',
    ],
    default  => []
  }

  # Which daemon to configure / install
  $with_slurmd    = false
  $with_slurmctld = false
  $with_slurmdbd  = false

  # Whether or not this module should configure firewall[d]
  $manage_firewall   = false
  # Whether or not this module should manage the accounting
  $manage_accounting = false

  # Configuration directory & file
  $configdir = $::operatingsystem ? {
    default => '/etc/slurm',
  }
  $logdir = $::operatingsystem ? {
    default => '/var/log/slurm'
  }
  # $piddir = $::operatingsystem ? {
    #   default => '/var/run/slurm',
    # }
  # Slurm controller save state directory
  $slurmctld_libdir = $::operatingsystem ? {
    default => '/var/lib/slurmctld',
  }
  $pluginsdir = 'plugstack.conf.d'

  $configdir_mode = '0755'

  # The below directory (absolute path) is used as a target for synchronizing the
  # Slurm configuration
  $shared_configdir = ''

  # Where the [git] control repository will be cloned
  $repo_basedir = '/usr/local/src/git'

  # $configdir_owner = $::operatingsystem ? {
    #   default => 'root',
    # }
  # $configdir_group = $::operatingsystem ? {
    #   default => 'root',
    # }

  ### Log directory
  # $logdir_mode = $::operatingsystem ? {
    #   default => '750',
    # }
  # $logdir_owner = $::operatingsystem ? {
    #   default => 'root',
    # }
  # $logdir_group = $::operatingsystem ? {
    #   default => 'adm',
    # }

  # PID for daemons
  # $piddir_mode = $::operatingsystem ? {
    #     default => '750',
    # }
  # $piddir_owner = $::operatingsystem ? {
    #     default => 'slurm',
    # }
  # $piddir_group = $::operatingsystem ? {
    #     default => 'adm',
    # }
  # $pidfile = $::operatingsystem ? {
    #     default => '/var/run/slurm/slurm.pid'
    # }

  ####################################################
  ### SLURM Configuration files (slurm.conf etc.)  ###
  ####################################################
  #
  # See https://slurm.schedmd.com/slurm.conf.html
  #
  $configfile              = 'slurm.conf'
  $configfile_mode         = '0644'

  # The name by which this Slurm managed cluster is known in the accounting database
  $clustername             = 'cluster'
  # Main / backup Slurm controllers
  $controlmachine          = $::hostname
  $controladdr             = ''
  $backupcontroller        = ''
  $backupaddr              = ''
  # Accounting storage slurmdbd server
  $accountingstoragehost   = $::hostname

  # Authentication method for communications between Slurm components.
  $authtype                = 'munge' # in [ 'none', 'munge' ]
  $authinfo                = ''
  $cryptotype              = 'munge' # in [ 'munge', 'openssl']

  # What level of association-based enforcement to impose on job submissions
  $accountingstoragetres   = ''
  $acct_storageenforce     = ['qos', 'limits', 'associations']
  $acct_gatherenergytype   = 'none'
  $batchstarttimeout       = 10
  $getenvtimeout           = 2
  $checkpointtype          = 'none'  # in ['blcr', 'none', 'ompi', 'poe']
  $completewait            = 0
  $corespecplugin          = 'none'
  $cpufreqdef              = undef   # or in ['Conservative', 'OnDemand', 'Performance', 'PowerSave']
  $cpufreqgovernors        = [ 'OnDemand', 'Performance' ]  # in above + 'UserSpace'
  $debugflags              = []

  $defmempercpu            = 0           # 0 = unlimited, mutually exclusive with $defmempernode
  $maxmempercpu            = 0           # 0 = unlimited
  $maxmempernode           = undef
  $messagetimeout          = 10
  $defmempernode           = undef       # 0 = unlimited, mutually exclusive with $defmempercpu

  $disablerootjobs         = true
  $enforcepartlimits       = 'ALL'
  $epilog                  = ''
  $epilogslurmctld         = ''
  $fastschedule            = 1           # in [ 0, 1, 2]
  $grestypes               = []          # list of generic resources to be managed
  $healthcheckinterval     = 30
  $healthchecknodestate    = 'ANY'       # in ['ALLOC', 'ANY','CYCLE','IDLE','MIXED']
  $healthcheckprogram      = ''
  $inactivelimit           = 0           # 0 = unlimited
  $jobacctgathertype       = 'cgroup'    # in [ "linux", "cgroup", "none"]
  $jobacctgatherfrequency  = 30
  $jobacctgatherparams     = ''          # in [ 'NoShared', 'UsePss', 'NoOverMemoryKill']
  $jobcheckpointdir        = ''
  $jobcomphost             = ''          # machine hosting the job completion database
  $jobcomploc              = 'slurmjobs' # where job completion records are written (DB name, filename...)
  $jobcomptype             = 'none'      # in ["none", "elasticsearch", "filetxt", "mysql", "script"]
  $jobcontainertype        = 'none'      # In ['cncu', 'none'] (CNCU = Compute Node Clean Up on Cray)
  $jobrequeue              = true
  $jobsubmitplugins        = [ 'lua' ]   #
  $killwait                = 30          # sec. interval given to a job's processes between the SIGTERM and SIGKILL
  $launchtype              = 'slurm'
  $licenses                = ''          # Specification of licenses
  $maildomain              = $::domain
  $mailprog                = '/bin/mail'
  $maxtaskspernode         = 512

  # Default type of MPI to be used. Srun may override this configuration parameter in any case.
  # in ['lam','mpich1_p4','mpich1_shmem','mpichgm','mpichmx','mvapich','none','openmpi','pmi2']
  $mpidefault              = 'none'   # See https://slurm.schedmd.com/mpi_guide.html
  $mpiparams               = ''

  #
  # Nodes
  # Format
  # { 'nodename' => 'CPUs=xx Sockets=xx...' }
  # OR
  # { 'nodename' => {
    #     comment => 'a comment',
    #     content =>'CPUs=xx Sockets=xx...',
    # }
  # }
$nodes                   = {}

$preemptmode             = [ 'REQUEUE' ] # in ['OFF','CANCEL','CHECKPOINT','GANG','REQUEUE','SUSPEND']
$preempttype             = 'qos'         # in ['none', 'partition_prio', 'qos']
$prioritydecayhalflife   = '5-0'         # aka 5 days
$priorityfavorsmall      = false
$priorityflags           = []            # in ['ACCRUE_ALWAYS','CALCULATE_RUNNING','DEPTH_OBLIVIOUS','FAIR_TREE','INCR_ONLY','MAX_TRES','SMALL_RELATIVE_TO_TIME']
$prioritymaxage          = '7-0'
$priorityweighttres      = ''
$prioritytype            = 'multifactor' # in ['basic', 'multifactor']
$priorityusageresetperiod = 'NONE'       # in ['NONE','NOW','DAILY','WEEKLY','MONTHLY','QUARTERLY','YEARLY']
$priorityweightage       = 0
$priorityweightfairshare = 0
$priorityweightjobsize   = 0
$priorityweightpartition = 0
$priorityweightqos       = 0
$privatedata             = []  # in ['accounts','cloud','jobs','nodes','partitions','reservations','usage','users']
$proctracktype           = 'cgroup'      # in ['cgroup', 'cray', 'linuxproc', 'lua', 'sgi_job','pgid']

$prolog                  = ''
$prologflags             = []
$prologslurmctld         = ''
$propagateresourcelimits = []
$propagateresourcelimits_except = [ 'MEMLOCK'] # see https://slurm.schedmd.com/faq.html#memlock
$resvoverrun             = 0
$resumetimeout           = 60
$resumeprogram           = ''
$suspendprogram          = ''
$suspendtimeout          = 0
$suspendtime             = 0
$suspendexcnodes         = ''
$suspendexcparts         = ''
$resumerate              = 300
# Controls when a DOWN node will be returned to service
$returntoservice         = 1  # in [0, 1, 2]
$statesavelocation       = '/var/lib/slurmctld'
$schedulertype           = 'backfill' # in ['backfill', 'builtin', 'hold']
$schedulerparameters     = []
$selecttype              = 'cons_res' # in ['bluegene','cons_res','cray','linear','serial' ]
$selecttype_params       = [ 'CR_Core_Memory', 'CR_CORE_DEFAULT_DIST_BLOCK' ]
# Log details
$slurmdbddebug           = 'info'
$slurmdbddebugsyslog     = ''
$slurmctlddebug          = 'info'
$slurmctlddebugsyslog    = ''
$slurmddebug             = 'info'
$slurmddebugsyslog       = ''
# Ports
$slurmctldport           = 6817
$slurmdport              = 6818
$slurmdbdport            = 6819
# Timeout
$slurmctldtimeout        = 120
$slurmdtimeout           = 300
$srunportrange           = '50000-53000'
$srunepilog              = ''
$srunprolog              = ''
$switchtype              = 'none' # in ['nrt', 'none']
$taskepilog              = ''
$taskplugin              = 'cgroup' # in ['affinity', 'cgroup','none']
$taskpluginparams        = ['cpusets']
$taskprolog              = ''
$tmpfs                   = '/tmp'
$tresbillingweights      = ''
$waittime                = 0
######
###### Hierarchical Network Topology description -- topology.conf
######
# Which topology plugin to be used for determining the network topology and
# optimizing job allocations to minimize network contention
# Elligible values
#   3d_torus  -- best-fit logic over three-dimensional topology
#   node_rank -- orders nodes based upon information a node_rank field in the
#       node record as generated by a select plugin. Slurm performs a best-fit
#       algorithm over those ordered nodes
#   none -- default for other systems, best-fit logic over one-dimensional topology
#   tree -- used for a hierarchical network as described in a topology.conf file
$topology_configfile = 'topology.conf'
$topology = none
# Format
# 'switchname' => {
  #              [comment => 'This will become a comment above the line',]
  #              nodes => '<nodes>',
  #              [linkspeed => '<speed>',]
  #        }
$topology_tree = {}

##################################
### Partition / QOS definition ###
##################################
# Default Partition / QoS. Format:
# '<name>' => {
  #     nodes         => n,           # Number of nodes
  #     default       => true|false,  # Default partition?
  #     hidden        => true|false,  # Hidden partition?
  #     allowgroups   => 'ALL|group[,group]*'
  #     allowaccounts => 'ALL|acct[,acct]*'
  #     allowqos      => 'ALL|qos[,qos]*'
  #     state         => 'UP|DOWN|DRAIN|INACTIVE'
  #     oversubscribe => 'EXCLUSIVE|FORCE|YES|NO' (replace :shared)
  #     #=== Time: Format is minutes, minutes:seconds, hours:minutes:seconds, days-hours,
  #             days-hours:minutes, days-hours:minutes:seconds or "UNLIMITED"
  #     default_time  => 'UNLIMITED|DD-HH:MM:SS',
  #     max_time      => 'UNLIMITED|DD-HH:MM:SS',
  #     #=== associated QoS config, named 'qos-<partition>' ===
  #     priority      => n           # QoS priority (default: 0)
  #     preempt       => 'qos-<name>
  # }
$partitions = {}
$qos        = {}

###
### Cgroup support -- cgroup.conf
###
# $cgroup_releaseagentdir = $::operatingsystem ? {
  #   default => '/etc/slurm/cgroup'
  # }
$cgroup_configfile = 'cgroup.conf'
$cgroup_automount  = true
$cgroup_mountpoint = $::operatingsystem ? {
  default          => '/sys/fs/cgroup'
}
### task/cgroup plugin ###
$cgroup_alloweddevices            = []    # if non-empty, cgroup_allowed_devices_file.conf
# will host the list of devices that need to be allowed by default for all the jobs
$cgroup_alloweddevices_configfile = 'cgroup_allowed_devices_file.conf'
$cgroup_allowedkmemspace          = undef   # amount of the allocated kernel memory
$cgroup_allowedramspace           = 100     # percentage of the allocated memory
$cgroup_allowedswapspace          = 0       # percentage of the swap space
$cgroup_constraincores            = true
$cgroup_constraindevices          = false   #  constrain the job's allowed devices based on GRES allocated resources.
$cgroup_constrainkmemspace        = true
$cgroup_constrainramspace         = true
$cgroup_constrainswapspace        = true
$cgroup_maxrampercent             = 100   # upper bound in percent of total RAM on the RAM constraint for a job.
$cgroup_maxswappercent            = 100   # upper bound (in percent of total RAM) on the amount of RAM+Swap
$cgroup_maxkmempercent            = 100   # upper bound in percent of total Kmem for a job.
$cgroup_minkmemspace              = 30    # lower bound (in MB) on the memory limits defined by AllowedKmemSpace.
$cgroup_minramspace               = 30    # lower bound (in MB) on the memory limits defined by AllowedRAMSpace & AllowedSwapSpace.
$cgroup_taskaffinity              = true  # This feature requires the Portable Hardware Locality (hwloc) library

###
### Generic RESource management -- gres.conf
###
$gres_configfile    = 'gres.conf'

#
# SPANK - Slurm Plug-in Architecture for Node and job (K)control
# See <https://slurm.schedmd.com/spank.html>
#
$pluginsdir_target = undef

#####################
### SLURM Daemons ###
#####################
# ensure the presence (or absence) of slurm
$ensure = 'present'

# Make sure the clocks, users and groups (UIDs and GIDs) are synchronized
# across the cluster.
# Slurm user / group identifiers
$username = 'slurm'
$uid      = 991
$group    = $username
$gid      = $uid
$home     = "/var/lib/${username}"
$comment  = 'SLURM workload manager'
$shell    = '/bin/bash'

# Slurmd associated services
$servicename = $::operatingsystem ? {
  default => 'slurmd'
}
$controller_servicename = $::operatingsystem ? {
  default => 'slurmctld'
}
$dbd_servicename = $::operatingsystem ? {
  default => 'slurmdbd'
}
# used for pattern in a service ressource
$processname            = $servicename
$controller_processname = $controller_servicename
$dbd_processname        = $dbd_servicename

$hasstatus = $::operatingsystem ? {
  /(?i-mx:ubuntu|debian)/        => false,
  /(?i-mx:centos|fedora|redhat)/ => true,
  default => true,
}
$hasrestart = $::operatingsystem ? {
  default => true,
}
# Whether to manage the slurm services
$service_manage = true

##########################################
### SLURM Sources and Building Process ###
##########################################
# Which group install is required to build the Slurm sources -- see slurm::build[::redhat]
# Makes only sense on yum-based systems
$groupinstall = $::osfamily ? {
  'Redhat' => 'Development tools',
  default  => undef
}
# Which version of Slurm to grab and build
$version = '17.11.12'

### SLURM Sources
# Checksum for the slurm source archive (empty means no check will be done)
$src_checksum      = '94fb13b509d23fcf9733018d6c961ca9'
$src_checksum_type = 'md5'
# From where the Slurm sources can be downloaded
$download_baseurl    = 'https://download.schedmd.com/slurm/'

$do_build            = true
$do_package_install  = true
# Where to place the sources
$srcdir = $::operatingsystem ? {
  default => '/usr/local/src'
}
### Slurm Build
# Where to place the builds of the sources (i.e. RPMs, debs...)
$builddir = $::osfamily ? {
  'Redhat' => '/root/rpmbuild', # rpmbuild _topdir Build directory
  default  => '/tmp/slurmbuild',
}
# Build options -- see https://github.com/SchedMD/slurm/blob/master/slurm.spec
$build_with = [
  #'auth_none',    # build auth-none RPM
  #'blcr',         # require blcr support
  #'bluegene',     # build bluegene RPM
  #'cray',         # build for a Cray system without ALPS
  #'cray_alps',    # build for a Cray system with ALPS
  #'cray_network', # build for a non-Cray system with a Cray network
  'lua',           # build Slurm lua bindings (proctrack only for now)
  'mysql',         # require mysql/mariadb support
  'openssl',       # require openssl RPM to be installed
  #'percs',        # build IBM PERCS RPM
  #'sgijob',       # build proctrack-sgi-job RPM
]
$build_without = [
  #'debug',        # don't compile with debugging symbols
  #'munge',        # don't build auth-munge RPM
  #'netloc',       # require netloc support
  #'pam',          # don't require pam-devel RPM to be installed
  #'readline',     # don't require readline-devel RPM to be installed
]
# Generated RPMs basenames, without versions and os specific suffixes.
# Exact filename will be on the form
#    <basename>-<version>-<n>.el7.centos.x86_64.rpm
#$rpm_basename = 'slurm'
$common_rpms_basename = [
  'slurm',           # Main RPM basename covering slurmd and slurmctld
  'slurm-contribs',  # Perl tool to print Slurm job state information
  'slurm-devel',     # Development package for Slurm
  #'slurm-lua',       # Slurm lua bindings
  #'slurm-munge',     # Slurm authentication and crypto implementation using Munge
  'slurm-pam_slurm', # PAM module for restricting access to compute nodes via Slurm
  'slurm-perlapi',   # Perl API to Slurm
  #'slurm-plugins',   # Slurm plugins (loadable shared objects)
]
$slurmdbd_rpms_basename = [
  'slurm-slurmdbd',  # Slurm database daemon
  #  'slurm-sql',       # Slurm SQL support
]
$slurmctld_rpms_basename = [
  'slurm-slurmctld',  # Slurm controller daemon - only on the head node
]
$slurmd_rpms_basename = [
  'slurm-slurmd',     # Slurm daemon - only on the compute nodes
]
$extra_rpms_basename = [
  'slurm-openlava',  # openlava/LSF wrappers for transitition from OpenLava/LSF to Slurm
  'slurm-torque',    # Torque/PBS wrappers for transitition from Torque/PBS to Slurm
]
$wrappers = []
####################################
### MUNGE authentication service ###
####################################
# see https://github.com/dun/munge
# We assume it will be used for shared key authentication, and the shared key
# can be provided to puppet via a URI.
$manage_munge     = true   # Whether or not this module should manage munge
# Munge user/group identifiers, and attributes for the munge user
$munge_username   = 'munge'
$munge_uid        = 992
$munge_group      = $munge_username
$munge_gid        = $munge_uid
$munge_home       = "/var/lib/${munge_username}"
$munge_comment    = "MUNGE Uid 'N' Gid Emporium"
$munge_shell      = '/sbin/nologin'
# Should the key be created if absent ?
$munge_create_key = true #false
$munge_key        = '/etc/munge/munge.key'
# Set the content of the DAEMON_ARGS variable
$munge_daemon_args = []
# Packages to install
$munge_package = $::operatingsystem ? {
  default => 'munge'
}
$munge_extra_packages = $::operatingsystem ? {
  /(?i-mx:ubuntu|debian)/        => [ 'libmunge-dev' ],
  /(?i-mx:centos|fedora|redhat)/ => [ 'munge-devel', 'munge-libs' ],
  default => [ ]
}
$munge_configdir = $::operatingsystem ? {
  default => '/etc/munge',
}
$munge_logdir = $::operatingsystem ? {
  default => '/var/log/munge',
}
$munge_piddir = $::operatingsystem ? {
  default => '/var/run/munge',
}
$munge_default_sysconfig = $::operatingsystem ? {
  /(?i-mx:ubuntu|debian)/ => '/etc/default/munge',
  default                 => '/etc/sysconfig/munge'
}
$munge_servicename = $::operatingsystem ? {
  default => 'munge'
}
$munge_processname = $::operatingsystem ? {
  default => 'munge'
}

##############################################
### Pluggable Authentication Modules (PAM) ###
##############################################
$manage_pam          = true   # Whether or not this module should manage pam
$use_pam             = true
$pam_allowed_users   = []
$pam_servicename     = 'slurm'
$pam_configfile      = '/etc/pam.d/slurm'
# Default content of /etc/pam.d/slurm
$pam_content         = template('slurm/pam_slurm.erb')
# Default ulimit -- update at least PAM MEMLOCK limits (required for MPI) +
# nproc (max number of processes)
$ulimits             = {
  'memlock' => 'unlimited',
  'stack'   => 'unlimited',
  'nproc'   => '10240',
}

# Source file for /etc/security/limits.d/slurm.conf
$pam_limits_source   = 'puppet:///modules/slurm/limits.memlock'
# Whether or not use the pam_slurm_adopt  module (to Adopt incoming
# connections into jobs) -- see
# https://github.com/SchedMD/slurm/tree/master/contribs/pam_slurm_adopt
$use_pam_slurm_adopt = false

######################
### SLURM Plugins  ###
######################
$job_submit_lua = 'job_submit.lua'

#####################################################
### SLURM DataBase Configuration (slurmdbd.conf)  ###
#####################################################
$dbd_configfile      = 'slurmdbd.conf'
$dbd_configfile_mode = '0400'
$archivedir          = '/tmp'
$archiveevents       = false # When purging events also archive them?
$archivejobs         = false # When purging jobs also archive them?
$archiveresv         = false # When purging reservations also archive them?
$archivesteps        = false # When purging steps also archive them?
$archivesuspend      = false # When purging suspend data also archive it?
$archivetxn          = false # When purging transaction data also archive it?
$archiveusage        = false # When purging usage data (Cluster, Association and WCKey) also archive it.
$commitdelay         = 0 # How many seconds between commits on a connection from a Slurmctld
$dbdhost             = 'localhost'
$dbdaddr             = 'localhost'
$dbdbackuphost       = ''
$storagehost         = 'localhost'
$storagebackuphost   = ''
$storageloc          = 'slurm'
$storageport         = 3306
$storagetype         = 'mysql'
$storageuser         = $username
$storagepass         = 'janIR4TvYoSEqNF94QM' # use 'openssl rand 14 -base64' for instance
$trackslurmctlddown  = false






}
